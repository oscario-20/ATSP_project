### R code from vignette source 'C:/Users/marca/OneDrive/2023/teaching_unterricht/Advanced Time Series in Finance (ATSF)/2023/27.02.2023/Rnw/slides_general_feedforward'

###################################################
### code chunk number 1: init
###################################################
# Load all relevant packages

rm(list=ls())


###################################################
### code chunk number 1: init
###################################################
# Load all relevant packages

inst_pack<-rownames(installed.packages())
if (!"fGarch"%in%inst_pack)
  install.packages("fGarch")
if (!"xts"%in%inst_pack)
  install.packages("xts")

if (!"neuralnet"%in%inst_pack)
  install.packages("neuralnet")
if (!"CaDENCE"%in%inst_pack)
  install.packages("CaDENCE")
if (!"quantmod"%in%inst_pack)
  install.packages("quantmod")


# Use iml package for classic XAI approaches see: https://cran.r-project.org/web/packages/iml/vignettes/intro.html

library(neuralnet)
library(fGarch)
library(xts)
library(CaDENCE)
library(quantmod)
library(PerformanceAnalytics)

setwd("C:/Users/Oscar/OneDrive - ZHAW/s - FS 2025/ATSF/03.03.2025 backpropagation general ff")
source(paste(getwd(),"/R/neuralnet_functions.R",sep=""))

recompute_results<-F

#############################################################################################################
# Load Bitcoin data
path.dat<-paste(getwd(),"/Data/",sep="")
path.results<-paste(getwd(),"/results/",sep="")
#path.dat<-paste(path.main,"/Exercises/Erste Woche/Data/",sep="")


load_data<-F

if (load_data)
{  
  getSymbols("BTC-USD")
  BTC<-get("BTC-USD")
  colnames(BTC)<-c("open","high","low","close","volume","adjusted")
  save(BTC,file=paste(path.dat,"BTC.Rdata",sep=""))
} else
{
  load(file=paste(path.dat,"BTC.Rdata",sep=""))
}

tail(BTC)
dat<-BTC

#-------------------
# Plot  data


# plot last, bid and ask in single figure names(dat)
par(mfrow=c(2,2))
plot(dat$close,col=1,main="Prices")
plot(log(dat$close),col=1,on=1,main="Log-prices")  #tail(dat$Bid)
plot(diff(log(dat$close)),col=1,on=1,main="Log-returns")
plot(log(dat$volume),col=1,on=1,main="Log-volumes")

#----------------------
# Specify target and explanatory data: we use first six lags based on above data analysis
x<-ret<-na.omit(diff(log(dat$close)))
x_level<-log(dat$close)
data_mat<-cbind(x,lag(x),lag(x,k=2),lag(x,k=3),lag(x,k=4),lag(x,k=5),lag(x,k=6))
# Check length of time series before na.exclude
dim(data_mat)
data_mat<-na.exclude(data_mat)
# Check length of time series after removal of NAs
dim(data_mat)
head(data_mat)
tail(data_mat)

#--------------------------------------------------------------------
# Specify in- and out-of-sample episodes
in_out_sample_separator<-"2018-01-01"

# Use original log-returns without scaling or transformation: xts-objects 
# These are used for computing trading performances
y_test_xts<-target_out<-data_mat[paste(in_out_sample_separator,"/",sep=""),1]
x_test_xts<-explanatory_out<-data_mat[paste(in_out_sample_separator,"/",sep=""),2:ncol(data_mat)]
y_train_xts<-target_out<-data_mat[paste("/",in_out_sample_separator,sep=""),1]
x_train_xts<-explanatory_out<-data_mat[paste("/",in_out_sample_separator,sep=""),2:ncol(data_mat)]
# Buy and hold benchmark: in and out-sample
bh_out<-cumsum(data_mat[paste(in_out_sample_separator,"/",sep=""),1])
bh_in<-cumsum(data_mat[paste("/",in_out_sample_separator,sep=""),1])
plot(bh_in)
plot(bh_out)

#-----------------------------------------------------------------------------
# Activation function: sigmoid (atan_not_sigmoid<-F) or atan (atan_not_sigmoid<-T)
# We select sigmoid for neuralnet-package (otherwise convergence is very difficult)
atan_not_sigmoid<-F

# Scaling data for neural net: depends on activation function!!!!!!!!!!!!!!!!
maxs <- apply(data_mat, 2, max)
mins <- apply(data_mat, 2, min)
# Transform data into [0,1]
scaled <- scale(data_mat, center = mins, scale = maxs - mins)
# If atan then transform to [-1,1]  
if (atan_not_sigmoid)
  scaled<-2*(scaled-0.5)  
apply(scaled,2,min)
apply(scaled,2,max)
#-----------------
# Train-test split
# The scaled data is uswd for parameter fitting
# The scaling depends on the activation function: in [0,1] for sigmoid and in [-1,1] for atan
# xts objects
train_set_xts <- scaled[paste("/",in_out_sample_separator,sep=""),]
test_set_xts <- scaled[paste(in_out_sample_separator,"/",sep=""),]
# as matrices
train_set<-as.matrix(train_set_xts)
test_set<-as.matrix(test_set_xts)

# These are scaled data for traing purposes: they are not xts-objects
x_train<-train_set[,-1]
y_train<-train_set[,1]

#-----------------------------------

colnames(train_set)<-paste("lag",0:(ncol(train_set)-1),sep="")
n <- colnames(train_set)
# Model: target is current bitcoin, all other variables are explanatory
f <- as.formula(paste("lag0 ~", paste(n[!n %in% "lag0"], collapse = " + ")))

# Set/fix the random seed
set.seed(1)
nn <- neuralnet(f,data=train_set,hidden=c(20,10),linear.output=T)


plot(nn,rep="best")




###################################################
### code chunk number 3: init
###################################################
# R-code: data transformation
# Activation function: sigmoid (atan_not_sigmoid<-F) or atan (atan_not_sigmoid<-T)
# Atan often leads to tighter fit (smaller MSE)
# We select atan (requires fewer epochs to converge)
atan_not_sigmoid<-T

# Scaling data for neural net: depends on activation function!!!!!!!!!!!!!!!!
maxs <- apply(data_mat, 2, max)
mins <- apply(data_mat, 2, min)

tail(data_mat)
# Transform data into [0,1]
scaled <- scale(data_mat, center = mins, scale = maxs - mins)
# If atan then transform to [-1,1]  
if (atan_not_sigmoid)
  scaled<-2*(scaled-0.5)  
apply(scaled,2,min)
apply(scaled,2,max)
#-----------------
# Train-test split
# The scaled data is used for parameter fitting
# The scaling depends on the activation function: in [0,1] for sigmoid and in [-1,1] for atan
# xts objects
train_set_xts <- scaled[paste("/",in_out_sample_separator,sep=""),]
test_set_xts <- scaled[paste(in_out_sample_separator,"/",sep=""),]
# as matrices
train_set<-as.matrix(train_set_xts)
test_set<-as.matrix(test_set_xts)
colnames(train_set)<-paste("lag",0:(ncol(train_set)-1),sep="")
n <- colnames(train_set)


# These are scaled data for traing purposes: they are not xts-objects
# Note that negative outliers of BTC are so strong that scaled data is mostly positive
x_train<-train_set[,-1]
y_train<-train_set[,1]





###################################################
### code chunk number 4: init
###################################################
# R-code: demo general feedforward

# A three hidden layer net
neuron_vec<-c(20,10,3)
# A relatively simple net
neuron_vec<-c(3,2)

# Plot the net: use neuralnet package
nn <- neuralnet(f,data=train_set,hidden=neuron_vec,linear.output=T)
plot(nn,rep="best")

# Output: non-linear (linear_output<-F) or linear (linear_output<-T)
linear_output<-F
# Activation function: sigmoid (atan_not_sigmoid<-F) or atan (atan_not_sigmoid<-T)
atan_not_sigmoid<-atan_not_sigmoid

# Explanatory data for training
x_train<-as.matrix(train_set[,2:ncol(train_set)])
# Target for training
y<-y_train<-as.matrix(train_set[,1],ncol=1)

y_train[1:10,]
dim(x_train)

hidden_neurons<-neuron_vec

list_layer_size<-layer_size<-getLayerSize(x_train, y_train, hidden_neurons)

# Layers and neurons per layer
list_layer_size

# Set the random seed
set.seed(1)
# Use random initializations for the weights; biases are initialized with zero
init_params <- initializeParameters(list_layer_size)

init_params

# Generate output of net given data x_train and above random parameters
fwd_prop <- forwardPropagation(x_train, init_params, layer_size,linear_output,atan_not_sigmoid)

output<-fwd_prop$A_list[[length(fwd_prop$A_list)]]
as.double(output)

# Compute MSE
cache<-fwd_prop

cost <- computeCost(y_train, fwd_prop)
cost


# Compute gradient for optimization: backpropagation

back_prop <- backwardPropagation(x_train, y_train, fwd_prop, init_params, layer_size,linear_output,atan_not_sigmoid)

grads<-back_prop

grads

# Check backpropagation: change parameters (not input x!!!)
# Check backpropagation for weights w
delta<-0.000001
# Layer
j<-2
# From neuron k
k<-1
# to neuron i
i<-2
init_params_modified<-init_params
init_params_modified$W_list[[j]][i,k]<-init_params$W_list[[j]][i,k]+delta
fwd_prop_modified <- forwardPropagation(x_train, init_params_modified, layer_size,linear_output,atan_not_sigmoid)
cost_modified <- computeCost(y_train, fwd_prop_modified)
# Check
(cost_modified-cost)/delta
grads$dW_list[[j]][i,k]


# Check backpropagation for bias b
delta<-0.0001
j<-2
i<-1
init_params_modified<-init_params
init_params_modified$b_list[[j]][i]<-init_params$b_list[[j]][i]+delta
fwd_prop_modified <- forwardPropagation(x_train, init_params_modified, layer_size,linear_output,atan_not_sigmoid)
cost_modified <- computeCost(y_train, fwd_prop_modified)
# Check
(cost_modified-cost)/delta
grads$db_list[[j]][i]




if (F)
{
for (i in 1:length(params$W_list))
print(dim(grads$dW_list[[i]] ))

for (i in 1:length(params$W_list))
  print(dim(updated_params$W_list[[i]] ))
}


#params<-init_params


update_params <- updateParameters(back_prop, init_params, learning_rate = 0.01)




###################################################
### code chunk number 5: init
###################################################
###############################################

# R-code: Train the model backpropagation
# 1. Use backpropagation


#----------------------
# How many iterations
epochs<-50
# Try learning-rates: look at plot of cost-history below (small learning_rate means very slow progress)
learning_rate<-0.001
#--
if (linear_output)
  learning_rate<-learning_rate/10
#--
hyper_list<-list(epochs=epochs,learning_rate=learning_rate,linear_output=linear_output,atan_not_sigmoid=atan_not_sigmoid,neuron_vec=neuron_vec)

set.seed(1)

# Train/Learn/Optimize
train_model <- trainModel(x_train, y_train, hyper_list=hyper_list)


# Plot optimization path
ts.plot(train_model$cost_hist)
# Last entry i.e. final MSE/criterion value
train_model$cost_hist[length(train_model$cost_hist)]



###################################################
### code chunk number 7: init
###################################################
# Try learning-rates: look at plot of cost-history below (small learning_rate means very slow progress)
learning_rate<-0.01
#--
if (linear_output)
  learning_rate<-learning_rate/10
#--
hyper_list<-list(epochs=epochs,learning_rate=learning_rate,linear_output=linear_output,atan_not_sigmoid=atan_not_sigmoid,neuron_vec=neuron_vec)

set.seed(1)

# Train/Learn/Optimize
train_model <- trainModel(x_train, y_train, hyper_list=hyper_list)


# Plot optimization path
ts.plot(train_model$cost_hist)
# Last entry i.e. final MSE/criterion value
train_model$cost_hist[length(train_model$cost_hist)]




###################################################
### code chunk number 9: init
###################################################
# Try learning-rates: look at plot of cost-history below (small learning_rate means very slow progress)
learning_rate<-0.3
#--
if (linear_output)
  learning_rate<-learning_rate/10
#--
hyper_list<-list(epochs=epochs,learning_rate=learning_rate,linear_output=linear_output,atan_not_sigmoid=atan_not_sigmoid,neuron_vec=neuron_vec)

set.seed(1)

# Train/Learn/Optimize
train_model <- trainModel(x_train, y_train, hyper_list=hyper_list)


# Plot optimization path
ts.plot(train_model$cost_hist)
# Last entry i.e. final MSE/criterion value
train_model$cost_hist[length(train_model$cost_hist)]






###################################################
### code chunk number 11: init
###################################################
# Try learning-rates: look at plot of cost-history below (small learning_rate means very slow progress)
learning_rate<-2
#--
if (linear_output)
  learning_rate<-learning_rate/10
#--
hyper_list<-list(epochs=epochs,learning_rate=learning_rate,linear_output=linear_output,atan_not_sigmoid=atan_not_sigmoid,neuron_vec=neuron_vec)

set.seed(1)

# Train/Learn/Optimize
train_model <- trainModel(x_train, y_train, hyper_list=hyper_list)


# Plot optimization path
ts.plot(train_model$cost_hist)
# Last entry i.e. final MSE/criterion value
train_model$cost_hist[length(train_model$cost_hist)]





###################################################
### code chunk number 13: init
###################################################
# R code: comparison with neuralnet
set.seed(1)
# Use mid-sized learning-rate
learning_rate<-0.1
#--
if (linear_output)
  learning_rate<-learning_rate/10
#--
hyper_list<-list(epochs=epochs,learning_rate=learning_rate,linear_output=linear_output,atan_not_sigmoid=atan_not_sigmoid,neuron_vec=neuron_vec)

# Train/Learn/Optimize
train_model <- trainModel(x_train, y_train, hyper_list=hyper_list)

# Plot optimization path
ts.plot(train_model$cost_hist)
# Last entry i.e. final MSE/criterion value
train_model$cost_hist[length(train_model$cost_hist)]

# Optimal parameters
updated_params<-train_model$updated_params
# Output of optimized net
fwd_prop <- forwardPropagation(x_train, updated_params, layer_size,linear_output,atan_not_sigmoid)
cache<-fwd_prop
output<-fwd_prop$A_list[[length(fwd_prop$A_list)]]
# MSE of optimized net
cost <- computeCost(y_train, fwd_prop)
# Check: this is the same as the final MSE above
cost

#-------------------------
# Compare with neuralnet
#   Select setseed for neuralnet package
setseed<-71
set.seed(setseed)

nn <- neuralnet(f,data=train_set,hidden=hyper_list$neuron_vec,linear.output=hyper_list$linear_output)
plot(nn)

# Compute net-output
pr.nn <- compute(nn,train_set[,2:ncol(train_set)])
predicted_scaled<-pr.nn$net.result

# Compute MSE neuralnet
mean((train_set[,1]-predicted_scaled)^2)
# Compare with above feedforward backpropagation
train_model$cost_hist[length(train_model$cost_hist)]





###################################################
### code chunk number 14: init
###################################################
###############################################
#R-code: Train the model nlminb

# 2. Use nlminb (not backpropagation)
#   Specify a 'simple' net because numerical convergence can be quite slow
#   Advantage of backpropagation: often faster


layer_size<-getLayerSize(x_train, y_train, hidden_neurons)

# Compute the total number of parameters: we do not distinguish weights/biases or layers here
# nlminb just takes a vector of 'unordered' parameters
parm_len<-compute_number_parameters(layer_size)

# Initialize parameters
set.seed(setseed)
parm<-parm_init<-0.1*rnorm(parm_len)
# No ordering according to layers, weights and biases are not properly identified
parm

x<-x_train
y<-y_train
# Numerical optimization in R

if (recompute_results)
{

  nlmin_obj<-nlminb(parm_init,optimize_nlminb_net,x=x,y=y,layer_size=layer_size,linear_output=linear_output,atan_not_sigmoid=atan_not_sigmoid)
  
  # Optimized criterion: compare with above MSEs
  nlmin_obj$objective
  
  # Optimal parameters
  parm_opt<-nlmin_obj$par
  # Distinguish parameters into weights and biases and arrange into layers
  parm_opt_list<-translate_Parameters(list_layer_size,parm_opt)
  parm_opt_list
  
  # Check: this number should correspond to the optimal criterion value above
  optimize_nlminb_net(parm_opt,x,y,layer_size,linear_output,atan_not_sigmoid)
}


###################################################
### code chunk number 15: init
###################################################
###############################################
#R-code: Train the model optim
# 3. Use optim (not backpropagation)
if (recompute_results)
{
  optim_obj<-optim(parm_init,optimize_nlminb_net,x=x,y=y,layer_size=layer_size,linear_output=linear_output,atan_not_sigmoid=atan_not_sigmoid,method="BFGS")
  
  optim_obj$value
}


###################################################
### code chunk number 16: init
###################################################
###############################################
#R-code: Train the model  rprop
# 3. Use optim (not backpropagation)

# Numerical optimization
if (recompute_results)
{
  rprop_obj<-rprop(parm_init, optimize_nlminb_net,x=x_train,y=y_train,layer_size=layer_size,linear_output=linear_output,atan_not_sigmoid=atan_not_sigmoid)
  
  # Optimized criterion value
  rprop_obj$value
  # Optimized parameters
  parm_opt<-rprop_obj$par
  
  # Here we 'translate' the above parameter-vector in terms of assignments to layers/weights/biases
  parm_opt<-translate_Parameters(list_layer_size,parm_opt)
  
  parm_opt
}




###################################################
### code chunk number 19: init
###################################################
####################################################
# R-code: Back to Bitcoin
# We DO NOT scale the data (unlike exercise bitcoin trading). But we use a linear output!!!!!
y_train<-target_in<-data_mat[paste("/",in_out_sample_separator,sep=""),1]
tail(target_in)
x_train<-explanatory_in<-data_mat[paste("/",in_out_sample_separator,sep=""),2:ncol(data_mat)]
tail(explanatory_in)
y_test<-target_out<-data_mat[paste(in_out_sample_separator,"/",sep=""),1]
tail(target_out)
x_test<-explanatory_out<-data_mat[paste(in_out_sample_separator,"/",sep=""),2:ncol(data_mat)]
tail(explanatory_out)

head(x_train)
tail(x_train)
head(y_train)
tail(y_train)
head(x_test)
tail(x_test)
head(y_test)
tail(y_test)
#-------------------------------------------------------------------------
# Settings
# Net architecture: one hidden with 100 neurons
neuron_vec<-c(100)
# Plot
set.seed(1)
nn <- neuralnet(f,data=train_set[1:200,],neuron_vec,linear.output=F)
plot(nn,rep="best")

# Linear output
#   Sigmoid output would be a problem for unscald data which can be negative
#   Either use linear_output<-T (then the output neuron is linear) or atan_not_sigmoid<-T (atan function can be negative in conrast to sigmoid)
linear_output<-T
# Sigmoid activation
atan_not_sigmoid<-F
# Optimization settings
epochs<-50
learning_rate<-0.3
if (linear_output)
  learning_rate<-learning_rate/10
#if (!atan_not_sigmoid)
#  learning_rate<-learning_rate*5
hyper_list<-list(epochs=epochs,learning_rate=learning_rate,linear_output=linear_output,atan_not_sigmoid=atan_not_sigmoid,neuron_vec=neuron_vec)

#-----------------
# Linear regression
# Lag 6 dominates
lm_obj<-lm(y_train~x_train)
summary(lm_obj)
mean(lm_obj$res^2)
mse_regression_out_sample<-mean((as.double(y_test)-lm_obj$coefficients[1+1:ncol(x_test)]%*%t(x_test))^2)
mse_regression_out_sample
#------------------
# Net unregularized and regularized
list_layer_size<-layer_size<-getLayerSize(x_train, y_train, neuron_vec)
list_layer_size
set.seed(1)

# Number of random nets (each setseed leads to another net)
anzsim<-10

# Train model
# No regularization
lambda<-0.

if (recompute_results)
{
  setseed<-0


  mplot_sign<-mplot_prop<-mplot_sign_reg<-mplot_prop_reg<-cumsum(y_test)
  out_mse<-out_reg_all<-NULL
  pb <- txtProgressBar(min = 1, max = anzsim, style = 3)
  for (i in 1:anzsim)#i<-4
  {
# Change seed for each pass-through
    setseed<-setseed+1

    compute_mse_original<-compute_net_func(x_train, y_train,x_test,y_test, hyper_list,lambda,setseed,layer_size)
# Check convergence    
    ts.plot(compute_mse_original$train_model$cost_hist)
# trading performances out-of-sample
    updated_params<-compute_mse_original$train_model$updated_params

# Out-of-sample output of optimized net
    fwd <- forwardPropagation(x_test, updated_params, layer_size,linear_output,atan_not_sigmoid)
    output<-as.double(fwd$A_list[[length(fwd$A_list)]])
    out_mse<-cbind(out_mse,output)
    perf_sign<-as.double(y_test)*sign(output)
    mplot_sign<-cbind(mplot_sign,cumsum(perf_sign))
    perf_prop<-as.double(y_test)*output/mean(abs(output))
    mplot_prop<-cbind(mplot_prop,cumsum(perf_prop))
    setTxtProgressBar(pb, i)



  }
  close(pb)
  save(mplot_sign,file=paste(path.results,"bit_sign_trade_mse",sep=""))
  save(mplot_prop,file=paste(path.results,"bit_prop_trade_mse",sep=""))
  save(out_mse,file=paste(path.results,"bit_out_mse",sep=""))


} else
{
  load(file=paste(path.results,"bit_sign_trade_mse",sep=""))
  load(file=paste(path.results,"bit_prop_trade_mse",sep=""))
  load(file=paste(path.results,"bit_out_mse",sep=""))
}

par(mfrow=c(1,1))
mplot<-as.matrix(mplot_sign)
colo<-rainbow(ncol(mplot))
plot(mplot[,1],main=paste("Log-Cum-Perf random nets ordinary MSE"),col=colo[1],type="l",axes=F,xlab="",ylab="",ylim=c(min(mplot),max(mplot)))
for (i in 2:ncol(mplot))
  lines(mplot[,i],col=colo[i])
lines(mplot[,1],col="black",lwd=4)
#  mtext(colnames(mplot)[1],line=-1,col=colo[1])
#  mtext(paste("Ideal lowpass with cutoff-length ",per_m,sep=""),line=-2,col=colo[3])
axis(1,at=1:nrow(mplot),labels=index(y_test))#length(index(data_obj$y_test))
axis(2)
box()




par(mfrow=c(1,1))
mplot<-as.matrix(mplot_prop)
colo<-rainbow(ncol(mplot))
plot(mplot[,1],main=paste("Log-Cum-Perf random nets ordinary MSE"),col=colo[1],type="l",axes=F,xlab="",ylab="",ylim=c(min(mplot),max(mplot)))
for (i in 2:ncol(mplot))
  lines(mplot[,i],col=colo[i])
lines(mplot[,1],col="black",lwd=4)
#  mtext(colnames(mplot)[1],line=-1,col=colo[1])
#  mtext(paste("Ideal lowpass with cutoff-length ",per_m,sep=""),line=-2,col=colo[3])
axis(1,at=1:nrow(mplot),labels=index(y_test))#length(index(data_obj$y_test))
axis(2)
box()




# Aggregate performance (of all random nets): regularized performs slightly worse
perf_agg_sign_mse<-apply(mplot_sign,1,mean)
perf_agg_prop_mse<-apply(mplot_prop,1,mean)
mplot<-cbind(cumsum(as.double(y_test)),as.double(perf_agg_sign_mse),as.double(perf_agg_prop_mse))
par(mfrow=c(1,1))
plot(mplot[,1],main=paste("Aggregate performances sign-rule (over random nets)"),col="black",type="l",axes=F,xlab="",ylab="",ylim=c(min(mplot),max(mplot)))
lines(mplot[,2],col="red")
lines(mplot[,3],col="blue")
mtext("Buy and hold",line=-1,col="black")
mtext("Sign",line=-2,col="red")
mtext("Proportional",line=-3,col="blue")
axis(1,at=1:nrow(mplot),labels=index(y_test))
axis(2)
box()






# Sharpe-ratio: annualize with sqrt(12) (monthly data)
sharpe_mat<-sqrt(365)*matrix(c(mean(as.double(y_test))/sd(as.double(y_test)),mean(diff(perf_agg_sign_mse),na.rm=T)/sd(diff(perf_agg_sign_mse),na.rm=T),mean(diff(perf_agg_prop_mse),na.rm=T)/sd(diff(perf_agg_prop_mse),na.rm=T)),nrow=1)
colnames(sharpe_mat)<-c("Buy-and-hold","Sign","Proportional")
sharpe_mat



