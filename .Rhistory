spy_xts <- xts(df[, -1], order.by = df$Date)
dim(spy_xts)
x<-na.omit(spy_xts$Overnight_log_Return)
head(x)
data_mat <- cbind(
x,
stats::lag(x, -1),
stats::lag(x, -2),
stats::lag(x, -3),
stats::lag(x, -4),
stats::lag(x, -5),
stats::lag(x, -6),
stats::lag(x, -7),
stats::lag(x, -8),
stats::lag(x, -9),
stats::lag(x, -10),
stats::lag(x, -11),
stats::lag(x, -12)
)
head(data_mat)
# Check length of time series before na.exclude
dim(data_mat)
data_mat<-na.exclude(data_mat)
# Check length of time series after removal of NAs
dim(data_mat)
head(data_mat)
tail(data_mat)
# Specify in- and out-of-sample episodes
in_out_sample_separator <- index(data_mat)[round(dim(data_mat)[1]/2)] # "2009-02-19"
in_out_sample_separator
target_in<-data_mat[paste("/",in_out_sample_separator,sep=""),1]
tail(target_in)
explanatory_in<-data_mat[paste("/",in_out_sample_separator,sep=""),2:ncol(data_mat)]
tail(explanatory_in)
target_out<-data_mat[paste(in_out_sample_separator,"/",sep=""),1]
head(target_out)
tail(target_out)
explanatory_out<-data_mat[paste(in_out_sample_separator,"/",sep=""),2:ncol(data_mat)]
train<-cbind(target_in,explanatory_in)
test<-cbind(target_out,explanatory_out)
head(test)
tail(test)
nrow(train)
nrow(test)
maxs <- apply(data_mat, 2, max)
mins <- apply(data_mat, 2, min)
# Transform data into [0,1]
scaled <- scale(data_mat, center = mins, scale = maxs - mins)
apply(scaled,2,min)
apply(scaled,2,max)
#-----------------
# 4.b
# Train-test split
train_set <- scaled[paste("/",in_out_sample_separator,sep=""),]
test_set <- scaled[paste(in_out_sample_separator,"/",sep=""),]
train_set<-as.matrix(train_set)
test_set<-as.matrix(test_set)
colnames(train_set)<-paste("lag",0:(ncol(train_set)-1),sep="")
n <- colnames(train_set)
#
f <- as.formula(paste("lag0 ~", paste(n[!n %in% "lag0"], collapse = " + ")))
tail(train_set)
# Set/fix the random seed
set.seed(3)
nn <- neuralnet(f,data=train_set,hidden=c(3,2),linear.output=F)
#------------------------------------
# 4.d (compare different realizations of the above net: train the net without specifying set.seed)
plot(nn)
# In sample performance
# 1. Without re-scaling: MSE based on transformed data
MSE.in.nn<-mean(((train_set[,1]-nn$net.result[[1]])*(max(data_mat[,1])-min(data_mat[,1])))^2)
MSE.in.nn
# 2. With re-scaling: MSE based on scale of original data
scaling_term<-(max(data_mat[,1])-min(data_mat[,1]))
MSE.in.nn<-mean(((train_set[,1]-nn$net.result[[1]])*scaling_term)^2)
MSE.in.nn
# Out-of-sample performance
# 1. Compute out-of-sample predictions based on transformed data
# Provide test-data to predict: use explanatory columns 2:ncol(test_set) (First column is forecast target)
pr.nn <- predict(nn,test_set[,2:ncol(test_set)])
tail(pr.nn)
predicted_scaled<-pr.nn
# Numbers are between 0 and 1
tail(predicted_scaled)
# Transform forecasts back to original data: rescale and shift by min(data_mat[,1])
predicted_nn <- predicted_scaled*scaling_term+min(data_mat[,1])
test.r <- test_set[,1]*scaling_term+min(data_mat[,1])
# Check: test.r is the same as test[,1]
test[,1]-test.r
# Calculating MSE
MSE.out.nn <- sum((test.r - predicted_nn)^2)/nrow(test_set)
# Compare in-sample and out-of-sample
c(MSE.in.nn,MSE.out.nn)
#--------------------------------
# 4.f Trading performance
perf_nn<-(sign(predicted_nn))*target_out
sharpe_nn<-sqrt(365)*mean(perf_nn,na.rm=T)/sqrt(var(perf_nn,na.rm=T))
par(mfrow=c(1,1))
plot(cumsum(perf_nn),main=paste("NN cumulated performances out-of-sample, sharpe=",round(sharpe_nn,2),sep=""))
tail(train_set)
tail(test_set)
head(train_set)
# Set/fix the random seed
set.seed(4)
nn <- neuralnet(f,data=train_set,hidden=c(20,10),linear.output=F)
# 1. Without re-scaling: MSE based on transformed data
MSE.in.nn<-mean(((train_set[,1]-nn$net.result[[1]])*(max(data_mat[,1])-min(data_mat[,1])))^2)
MSE.in.nn
# 2. With re-scaling: MSE based on scale of original data
scaling_term<-(max(data_mat[,1])-min(data_mat[,1]))
MSE.in.nn<-mean(((train_set[,1]-nn$net.result[[1]])*scaling_term)^2)
MSE.in.nn
pr.nn <- predict(nn,test_set[,2:ncol(test_set)])
predicted_scaled<-pr.nn
# Numbers are between 0 and 1
tail(predicted_scaled)
# Transform forecasts back to original data: rescale and shift by min(data_mat[,1])
predicted_nn <- predicted_scaled*scaling_term+min(data_mat[,1])
test.r <- test_set[,1]*scaling_term+min(data_mat[,1])
# Check: test.r is the same as test[,1]
test[,1]-test.r
# Calculating MSE
MSE.out.nn <- sum((test.r - predicted_nn)^2)/nrow(test_set)
# Compare in-sample and out-of-sample
c(MSE.in.nn,MSE.out.nn)
#--------------------------------
# Trading performance
perf_nn<-(sign(predicted_nn))*target_out
sharpe_nn<-sqrt(365)*mean(perf_nn,na.rm=T)/sqrt(var(perf_nn,na.rm=T))
par(mfrow=c(1,1))
plot(cumsum(perf_nn),main=paste("NN cumulated performances out-of-sample,nn(20,10), sharpe=",round(sharpe_nn,2),sep=""))
rm(list=ls())
library(neuralnet)
library(fGarch)
library(xts)
library(quantmod)
# Lade die Pakete
library(quantmod)
library(ggplot2)
library(dplyr)
# ---- 1️⃣ Lade SPY-Daten von Yahoo Finance ----
symbol <- "SPY"
getSymbols(symbol, src = "yahoo", from = "1993-01-01", to = Sys.Date(), auto.assign = TRUE)
df <- get(symbol)
head(df)
# Erstelle Dataframe mit relevanten Spalten
df <- data.frame(Date = index(df),
Open = as.numeric(Op(df)),
Close = as.numeric(Cl(df)))
# Berechne Renditen für alle drei Strategien
df <- df %>%
mutate(Open_log = log(Open),
Close_log = log(Close)) %>%
mutate(Overnight_log_Return = (Open_log / lag(Close_log)) - 1,   # Rendite: Buy Close, Sell Next Open
Intraday_log_Return = (Close_log / Open_log) - 1,         # Rendite: Buy Open, Sell Close
Daily_log_Return = (Close_log / lag(Close_log)) - 1)      # Rendite: Buy and Hold (tägliche Rendite)
len_before <- dim(df)[1]
# Entferne NA-Werte
df <- na.omit(df)
len_after <- dim(df)[1]
# number of removed rows
len_before - len_after
head(df)
x_fit<-as.ts(na.omit(df$Overnight_log_Return))
# GARCH(1,1)
y.garch_11<-garchFit(~garch(1,1),data=x_fit,delta=2,include.delta=F,include.mean=F,trace=F)
ts.plot(x_fit)
lines(y.garch_11@sigma.t,col="red")
standard_residuals<-y.garch_11@residuals/y.garch_11@sigma.t
ts.plot(standard_residuals)
par(mfrow=c(2,1))
acf(x_fit,main="Acf log-returns",ylim=c(0,0.1))
acf(standard_residuals,main="Acf standardized residuals GARCH(1,1)",ylim=c(0,0.1))
spy_xts <- xts(df[, -1], order.by = df$Date)
dim(spy_xts)
x<-na.omit(spy_xts$Overnight_log_Return)
head(x)
data_mat <- cbind(
x,
stats::lag(x, -1),
stats::lag(x, -2),
stats::lag(x, -3),
stats::lag(x, -4),
stats::lag(x, -5),
stats::lag(x, -6),
stats::lag(x, -7),
stats::lag(x, -8),
stats::lag(x, -9),
stats::lag(x, -10),
stats::lag(x, -11),
stats::lag(x, -12),
stats::lag(x, -13)
)
head(data_mat)
# Check length of time series before na.exclude
dim(data_mat)
data_mat<-na.exclude(data_mat)
# Check length of time series after removal of NAs
dim(data_mat)
head(data_mat)
tail(data_mat)
# Specify in- and out-of-sample episodes
in_out_sample_separator <- index(data_mat)[round(dim(data_mat)[1]*0.8)] # "2009-02-19"
in_out_sample_separator
in_out_sample_separator
target_in<-data_mat[paste("/",in_out_sample_separator,sep=""),1]
tail(target_in)
explanatory_in<-data_mat[paste("/",in_out_sample_separator,sep=""),2:ncol(data_mat)]
tail(explanatory_in)
target_out<-data_mat[paste(in_out_sample_separator,"/",sep=""),1]
head(target_out)
tail(target_out)
explanatory_out<-data_mat[paste(in_out_sample_separator,"/",sep=""),2:ncol(data_mat)]
train<-cbind(target_in,explanatory_in)
test<-cbind(target_out,explanatory_out)
head(test)
tail(test)
nrow(train)
nrow(test)
maxs <- apply(data_mat, 2, max)
mins <- apply(data_mat, 2, min)
# Transform data into [0,1]
scaled <- scale(data_mat, center = mins, scale = maxs - mins)
apply(scaled,2,min)
apply(scaled,2,max)
# Train-test split
train_set <- scaled[paste("/",in_out_sample_separator,sep=""),]
test_set <- scaled[paste(in_out_sample_separator,"/",sep=""),]
train_set<-as.matrix(train_set)
test_set<-as.matrix(test_set)
colnames(train_set)<-paste("lag",0:(ncol(train_set)-1),sep="")
n <- colnames(train_set)
#
f <- as.formula(paste("lag0 ~", paste(n[!n %in% "lag0"], collapse = " + ")))
tail(train_set)
# Set/fix the random seed
set.seed(4)
nn <- neuralnet(f,data=train_set,hidden=c(20,10),linear.output=F)  # ca. 15 min training
setwd("C:/Users/Oscar/OneDrive - ZHAW/s - FS 2025/ATSF/Project/ATSP_project")
save(nn, file = "nn_model_20_10.RData")
# In sample performance
# 1. Without re-scaling: MSE based on transformed data
MSE.in.nn<-mean(((train_set[,1]-nn$net.result[[1]])*(max(data_mat[,1])-min(data_mat[,1])))^2)
MSE.in.nn
# 2. With re-scaling: MSE based on scale of original data
scaling_term<-(max(data_mat[,1])-min(data_mat[,1]))
MSE.in.nn_scaling_term<-mean(((train_set[,1]-nn$net.result[[1]])*scaling_term)^2)
MSE.in.nn_scaling_term
# Out-of-sample performance
# 1. Compute out-of-sample predictions based on transformed data
# Provide test-data to predict: use explanatory columns 2:ncol(test_set) (First column is forecast target)
pr.nn <- predict(nn,test_set[,2:ncol(test_set)])
predicted_scaled<-pr.nn
# Numbers are between 0 and 1
tail(predicted_scaled)
# Transform forecasts back to original data: rescale and shift by min(data_mat[,1])
predicted_nn <- predicted_scaled*scaling_term+min(data_mat[,1])
test.r <- test_set[,1]*scaling_term+min(data_mat[,1])
# Check: test.r is the same as test[,1]
test[,1]-test.r
# Calculating MSE
MSE.out.nn <- sum((test.r - predicted_nn)^2)/nrow(test_set)
# Compare in-sample and out-of-sample
c(MSE.in.nn,MSE.out.nn)
#--------------------------------
# Trading performance
perf_nn<-(sign(predicted_nn))*target_out
sharpe_nn<-sqrt(365)*mean(perf_nn,na.rm=T)/sqrt(var(perf_nn,na.rm=T))
par(mfrow=c(1,1))
plot(cumsum(perf_nn),main=paste("NN cumulated performances out-of-sample,nn(20,10), sharpe=",round(sharpe_nn,2),sep=""))
# Numbers are between 0 and 1
tail(predicted_scaled)
tail(predicted_nn)
exp(tail(predicted_nn))
head(target_out)
exp(head(target_out))
# Erstelle Dataframe mit relevanten Spalten
symbol <- "SPY"
getSymbols(symbol, src = "yahoo", from = "1993-01-01", to = Sys.Date(), auto.assign = TRUE)
df <- get(symbol)
df <- data.frame(Date = index(df),
Open = as.numeric(Op(df)),
Close = as.numeric(Cl(df)))
# Berechne Renditen für alle drei Strategien
df <- df %>%
mutate(Open_log = log(Open),
Close_log = log(Close)) %>%
mutate(Overnight_log_Return = Open_log - lag(Close_log),# Rendite: Buy Close, Sell Next Open
Intraday_log_Return = Close_log - Open_log,# Rendite: Buy Open, Sell Close
Daily_log_Return = Close_log - lag(Close_log)) # Rendite: Buy and Hold (tägliche Rendite)
len_before <- dim(df)[1]
# Entferne NA-Werte
df <- na.omit(df)
len_after <- dim(df)[1]
# number of removed rows
len_before - len_after
head(df)
###############################################
# Fit a GARCH to log-returns of 'Overnight_log_Return'
x_fit<-as.ts(na.omit(df$Overnight_log_Return))
# GARCH(1,1)
y.garch_11<-garchFit(~garch(1,1),data=x_fit,delta=2,include.delta=F,include.mean=F,trace=F)
ts.plot(x_fit)
lines(y.garch_11@sigma.t,col="red")
#-----------------
standard_residuals<-y.garch_11@residuals/y.garch_11@sigma.t
ts.plot(standard_residuals)
#-----------------
par(mfrow=c(2,1))
acf(x_fit,main="Acf log-returns",ylim=c(0,0.1))
acf(standard_residuals,main="Acf standardized residuals GARCH(1,1)",ylim=c(0,0.1))
# acf and residuals acf sugest a dependency on lag 13, also on lag 29?
# Specify target and explanatory data: we use first 13 lags based on above data analysis
# convert data to xts
spy_xts <- xts(df[, -1], order.by = df$Date)
dim(spy_xts)
x<-na.omit(spy_xts$Overnight_log_Return)
head(x)
data_mat <- cbind(
x,
stats::lag(x, -1),
stats::lag(x, -2),
stats::lag(x, -3),
stats::lag(x, -4),
stats::lag(x, -5),
stats::lag(x, -6),
stats::lag(x, -7),
stats::lag(x, -8),
stats::lag(x, -9),
stats::lag(x, -10),
stats::lag(x, -11),
stats::lag(x, -12),
stats::lag(x, -13)
)
head(data_mat)
# Check length of time series before na.exclude
dim(data_mat)
data_mat<-na.exclude(data_mat)
# Check length of time series after removal of NAs
dim(data_mat)
head(data_mat)
tail(data_mat)
# Specify in- and out-of-sample episodes
in_out_sample_separator <- index(data_mat)[round(dim(data_mat)[1]*0.8)] # "2018-10-08"
in_out_sample_separator
target_in<-data_mat[paste("/",in_out_sample_separator,sep=""),1]
tail(target_in)
explanatory_in<-data_mat[paste("/",in_out_sample_separator,sep=""),2:ncol(data_mat)]
tail(explanatory_in)
target_out<-data_mat[paste(in_out_sample_separator,"/",sep=""),1]
head(target_out)
tail(target_out)
explanatory_out<-data_mat[paste(in_out_sample_separator,"/",sep=""),2:ncol(data_mat)]
train<-cbind(target_in,explanatory_in)
test<-cbind(target_out,explanatory_out)
head(test)
tail(test)
nrow(train)
nrow(test)
#-------------------------------------------------------------------------------
#  Prepare the datasets
# Scaling data for the NN
maxs <- apply(data_mat, 2, max)
mins <- apply(data_mat, 2, min)
# Transform data into [0,1]
scaled <- scale(data_mat, center = mins, scale = maxs - mins)
apply(scaled,2,min)
apply(scaled,2,max)
#-----------------
# Train-test split
train_set <- scaled[paste("/",in_out_sample_separator,sep=""),]
test_set <- scaled[paste(in_out_sample_separator,"/",sep=""),]
train_set<-as.matrix(train_set)
test_set<-as.matrix(test_set)
#-----------------------------------
colnames(train_set)<-paste("lag",0:(ncol(train_set)-1),sep="")
n <- colnames(train_set)
#
f <- as.formula(paste("lag0 ~", paste(n[!n %in% "lag0"], collapse = " + ")))
tail(train_set)
tail(train_set)
############################################----------------------------------------------------------------
# Set/fix the random seed
set.seed(3)
# overnigth-returns strategy
# Erstelle Dataframe mit relevanten Spalten
symbol <- "SPY"
getSymbols(symbol, src = "yahoo", from = "1993-01-01", to = Sys.Date(), auto.assign = TRUE)
df <- get(symbol)
df <- data.frame(Date = index(df),
Open = as.numeric(Op(df)),
Close = as.numeric(Cl(df)))
# Berechne Renditen für alle drei Strategien
df <- df %>%
mutate(Open_log = log(Open),
Close_log = log(Close)) %>%
mutate(Overnight_log_Return = Open_log - lag(Close_log),# Rendite: Buy Close, Sell Next Open
Intraday_log_Return = Close_log - Open_log,# Rendite: Buy Open, Sell Close
Daily_log_Return = Close_log - lag(Close_log)) # Rendite: Buy and Hold (tägliche Rendite)
len_before <- dim(df)[1]
# Entferne NA-Werte
df <- na.omit(df)
len_after <- dim(df)[1]
# number of removed rows
len_before - len_after
head(df)
###############################################
# Fit a GARCH to log-returns of 'Overnight_log_Return'
x_fit<-as.ts(na.omit(df$Overnight_log_Return))
# GARCH(1,1)
y.garch_11<-garchFit(~garch(1,1),data=x_fit,delta=2,include.delta=F,include.mean=F,trace=F)
ts.plot(x_fit)
lines(y.garch_11@sigma.t,col="red")
#-----------------
standard_residuals<-y.garch_11@residuals/y.garch_11@sigma.t
ts.plot(standard_residuals)
#-----------------
par(mfrow=c(2,1))
acf(x_fit,main="Acf log-returns",ylim=c(0,0.1))
acf(standard_residuals,main="Acf standardized residuals GARCH(1,1)",ylim=c(0,0.1))
spy_xts <- xts(df[, -1], order.by = df$Date)
dim(spy_xts)
x<-na.omit(spy_xts$Overnight_log_Return)
head(x)
data_mat <- cbind(
x,
stats::lag(x, -1),
stats::lag(x, -2),
stats::lag(x, -3),
stats::lag(x, -4),
stats::lag(x, -5),
stats::lag(x, -6),
stats::lag(x, -7),
stats::lag(x, -8),
stats::lag(x, -9),
stats::lag(x, -10),
stats::lag(x, -11),
stats::lag(x, -12),
stats::lag(x, -13)
)
head(data_mat)
# Check length of time series before na.exclude
dim(data_mat)
data_mat<-na.exclude(data_mat)
# Check length of time series after removal of NAs
dim(data_mat)
head(data_mat)
tail(data_mat)
# Specify in- and out-of-sample episodes
in_out_sample_separator <- index(data_mat)[round(dim(data_mat)[1]*0.8)] # "2018-10-08"
in_out_sample_separator
target_in<-data_mat[paste("/",in_out_sample_separator,sep=""),1]
tail(target_in)
explanatory_in<-data_mat[paste("/",in_out_sample_separator,sep=""),2:ncol(data_mat)]
tail(explanatory_in)
target_out<-data_mat[paste(in_out_sample_separator,"/",sep=""),1]
head(target_out)
tail(target_out)
explanatory_out<-data_mat[paste(in_out_sample_separator,"/",sep=""),2:ncol(data_mat)]
train<-cbind(target_in,explanatory_in)
test<-cbind(target_out,explanatory_out)
head(test)
tail(test)
nrow(train)
nrow(test)
maxs <- apply(data_mat, 2, max)
mins <- apply(data_mat, 2, min)
# Transform data into [0,1]
scaled <- scale(data_mat, center = mins, scale = maxs - mins)
apply(scaled,2,min)
apply(scaled,2,max)
# Train-test split
train_set <- scaled[paste("/",in_out_sample_separator,sep=""),]
test_set <- scaled[paste(in_out_sample_separator,"/",sep=""),]
train_set<-as.matrix(train_set)
test_set<-as.matrix(test_set)
colnames(train_set)<-paste("lag",0:(ncol(train_set)-1),sep="")
n <- colnames(train_set)
#
f <- as.formula(paste("lag0 ~", paste(n[!n %in% "lag0"], collapse = " + ")))
tail(train_set)
# Set/fix the random seed
set.seed(4)
nn <- neuralnet(f,data=train_set,hidden=c(20,10),linear.output=F)  # ca. 1 std training
dim(train_set)
setwd("C:/Users/Oscar/OneDrive - ZHAW/s - FS 2025/ATSF/Project/ATSP_project")
save(nn, file = "nn_model_20_10.RData")
# In sample performance
# 1. Without re-scaling: MSE based on transformed data
MSE.in.nn<-mean(((train_set[,1]-nn$net.result[[1]])*(max(data_mat[,1])-min(data_mat[,1])))^2)
MSE.in.nn
# 2. With re-scaling: MSE based on scale of original data
scaling_term<-(max(data_mat[,1])-min(data_mat[,1]))
MSE.in.nn_scaling_term<-mean(((train_set[,1]-nn$net.result[[1]])*scaling_term)^2)
MSE.in.nn_scaling_term
##################################################
# Out-of-sample performance
# 1. Compute out-of-sample predictions based on transformed data
# Provide test-data to predict: use explanatory columns 2:ncol(test_set) (First column is forecast target)
pr.nn <- predict(nn,test_set[,2:ncol(test_set)])
predicted_scaled<-pr.nn
# Numbers are between 0 and 1
tail(predicted_scaled)
# Transform forecasts back to original data: rescale and shift by min(data_mat[,1])
predicted_nn <- predicted_scaled*scaling_term+min(data_mat[,1])
test.r <- test_set[,1]*scaling_term+min(data_mat[,1])
# Check: test.r is the same as test[,1]
# test[,1]-test.r
# Calculating MSE
MSE.out.nn <- sum((test.r - predicted_nn)^2)/nrow(test_set)
# Compare in-sample and out-of-sample
c(MSE.in.nn,MSE.out.nn)
